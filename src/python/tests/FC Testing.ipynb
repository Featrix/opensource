{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Config for testing \n",
    "\n",
    "## Pick a config - default is to use the dev admin keys and admin account on local host "
   ],
   "id": "c85766193c39833"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T17:37:44.260061Z",
     "start_time": "2024-04-26T17:37:44.092513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List \n",
    "import os, sys, time \n",
    "from pathlib import Path \n",
    "data_files = Path(os.getcwd()) / \"data\"\n",
    "\n",
    "\n",
    "# LOCALHOST Testing \n",
    "# Dev bypass creds for localhost - doesn't depend on creating an api key \n",
    "os.environ['FEATRIX_CLIENT_ID'] = 'bd5ec45d-1c22-49fb-9b14-b3b13b428c68'\n",
    "os.environ['FEATRIX_CLIENT_SECRET'] = '4e7cddd7-dbdc-4f90-9a71-4b54cdec754e'\n",
    "\n",
    "target_api_server = \"http://localhost:3001\"\n",
    "allow_unencrypted_http = True \n",
    "\n",
    "# STAGE Testing -- you will need an API Key to be in ~/.featrix.key \n",
    "# target_api_server = \"https://stage.featrix.com\"\n",
    "# allow_unencrypted_http = False\n",
    "\n",
    "# PRODUCTION Testing -- you will need an API key in ~/.featrix.key or set it below \n",
    "# target_api_server = \"https://app.featrix.com\"\n",
    "# allow_unencrypted_http = False\n",
    "\n",
    "# If you have a key in the key file:\n",
    "# if 'FEATRIX_CLIENT_ID' in os.environ:\n",
    "#     del os.environ['FEATRIX_CLIENT_ID']\n",
    "# if 'FEATRIX_CLIENT_SECRET' in os.environ:\n",
    "#     del os.environ['FEATRIX_CLIENT_SECRET']\n",
    "\n",
    "# Otherwise be sure that you have FEATRIX_CLIENT_ID and FEATRIX_CLIENT_SECRET set in the process that started the notebook \n",
    "\n",
    "import featrixclient as ft\n",
    "\n",
    "FEATRIX_CLIENT_ID     = os.environ.get('FEATRIX_CLIENT_ID')\n",
    "FEATRIX_CLIENT_SECRET = os.environ.get('FEATRIX_CLIENT_SECRET')\n",
    "fc = ft.networkclient.new_client(\n",
    "                target_api_server,\n",
    "                client_id=FEATRIX_CLIENT_ID,\n",
    "                client_secret=FEATRIX_CLIENT_SECRET,\n",
    "                allow_unencrypted_http=allow_unencrypted_http,  # DEBUG\n",
    ")\n"
   ],
   "id": "e15074b529009305",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Create Neural Function",
   "id": "1b1d87e9c6ad0641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "wh_data = data_files / \"weight-height.csv\"\n",
    "fc.create_project(\"NNF WeightHeight Test 1\")\n",
    "fc.upload_file(wh_data, associate=True)\n",
    "# This will create an es and a model -- because we are waiting for completion, it will also wait for the upload to be ready for training.  Otherwise we could use:\n",
    "# while self.current_project.ready() is False: \n",
    "#     time.sleep(5)\n",
    "nf, es_job, nf_job = fc.create_neural_function(target_fields=\"Gender\", wait_for_completion=True)\n",
    "# Since we waited for completion, the nf will be ready for predicitons. If we hadn't we would need to watch the two jobs for job.finished and job.error \n",
    "# Now do a prediction \n",
    "query = {'Weight': 170}\n",
    "x = nf.predict(query)\n",
    "print(f\"Prediction query {query}: {x}\")\n",
    "fc.display_embedding_explorer()\n",
    "\n"
   ],
   "id": "275588c2600b2d55",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2/2:  Running: training; 100.0% complete; epoch = 75/75; batch = 250/250; current loss = 0.345, validation loss: 0.3160651922225952\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "from_job",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m fc\u001B[38;5;241m.\u001B[39mupload_file(wh_data, associate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# This will create an es and a model -- because we are waiting for completion, it will also wait for the upload to be ready for training.  Otherwise we could use:\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# while self.current_project.ready() is False: \u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#     time.sleep(5)\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m nf, es_job, nf_job \u001B[38;5;241m=\u001B[39m \u001B[43mfc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_neural_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_fields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGender\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwait_for_completion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Since we waited for completion, the nf will be ready for predicitons. If we hadn't we would need to watch the two jobs for job.finished and job.error \u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Now do a prediction \u001B[39;00m\n\u001B[1;32m     10\u001B[0m query \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWeight\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m170\u001B[39m}\n",
      "File \u001B[0;32m~/src/fc_os/src/python/featrixclient/networkclient.py:642\u001B[0m, in \u001B[0;36mcreate_neural_function\u001B[0;34m(self, target_fields, project, credit_budget, files, wait_for_completion)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredictions\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 642\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FeatrixException(\n\u001B[1;32m    643\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere is no current model, can not retrieve past predictions\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    644\u001B[0m         )\n\u001B[1;32m    645\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_model\u001B[38;5;241m.\u001B[39mpredictions()\n",
      "File \u001B[0;32m~/.python/fc_os/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:242\u001B[0m, in \u001B[0;36mModelMetaclass.__getattr__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m rebuilt_validator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    240\u001B[0m             \u001B[38;5;66;03m# In this case, a validator was built, and so `__pydantic_core_schema__` should now be set\u001B[39;00m\n\u001B[1;32m    241\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__pydantic_core_schema__\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 242\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(item)\n",
      "\u001B[0;31mAttributeError\u001B[0m: from_job"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Neural Function Single Line",
   "id": "80c05ecbbc08445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "animals_small = data_files / \"animals-1k.csv\"\n",
    "\n",
    "# This will create a project named CatDogSmall, make it the current project and wait for the training to complete. It will return the es and job \n",
    "es, job = fc.create_neural_function(target_fields=\"Animal\", project=\"CatDogSmall\", files=[animals_small], wait_for_completion=True)\n",
    "# This will display the embedding explorer for the current projects only embedding space.  We could also call via:\n",
    "# fc.display_embedding_explorer(embedding_space=es)\n",
    "fc.display_embedding_explorer()"
   ],
   "id": "ddfaea6c3dfde5b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Embedding Space ",
   "id": "80d20dc26da4b761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "animals = data_files / \"animals.csv\"\n",
    "\n",
    "# fc.create_project(\"myfriends2\")\n",
    "# fc.upload_file(f1, associate=True)\n",
    "\n",
    "\n",
    "\n",
    "#fc.create_neural_function(target_fields=\"hex_code\", wait_for_completion=True)\n",
    "fc.create_project(\"myfriends3\")\n",
    "fc.upload_file(animals, associate=True)\n",
    "while not fc.current_project.ready():\n",
    "    print(f\"Waiting for upload in project {fc.current_project.name} to be processed...\")\n",
    "es, job = fc.create_embedding_space(name=\"Animals-large-dataset\")\n",
    "while job.finished is not True:\n",
    "    print(f\"Waiting for es {es.name} to be finished training...(sleep 10)\")\n",
    "    time.sleep(10)\n",
    "    job = job.check()\n",
    "es = es.by_id(es.id)\n",
    "fc.display_embedding_explorer(embedding_space=es)"
   ],
   "id": "ad64fe81537b3c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Embedding Space Short",
   "id": "7d96066180941230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gh_data = data_files / \"gh-train.csv\"\n",
    "hw_data = data_files / \"hw-train.csv\"\n",
    "\n",
    "es, job = fc.create_embedding_space(name=\"GH-HW Embedding\", project=\"GH Train Test Project\", files=[gh_data, hw_data], wait_for_completion=True)\n",
    "fc.display_embedding_explorer(embedding_space=es)"
   ],
   "id": "c78ac9528fa49580",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Featrix CSV Loader ",
   "id": "f6fecbf4b7a7501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  -*- coding: utf-8 -*-\n",
    "#\n",
    "#  Copyright (c) 2024 Featrix, Inc, All Rights Reserved\n",
    "#\n",
    "#  Proprietary and Confidential.  Unauthorized use, copying or dissemination\n",
    "#  of these materials is strictly prohibited.\n",
    "#\n",
    "import csv\n",
    "import os\n",
    "import traceback\n",
    "from csv import Dialect\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_bad_line_number(file_path: Path | str = None, buffer: bytes | str = None):\n",
    "    try:\n",
    "        if file_path:\n",
    "            buffer = file_path.read_text()\n",
    "\n",
    "        reader = csv.reader(buffer)\n",
    "        linenumber = 1\n",
    "        try:\n",
    "            for row in reader:\n",
    "                linenumber += 1\n",
    "        except Exception as e:\n",
    "            return linenumber\n",
    "    except:\n",
    "        pass\n",
    "    return -1\n",
    "\n",
    "\n",
    "# A wrapper for dealing with CSV files.\n",
    "def featrix_wrap_pd_read_csv(\n",
    "    file_path: str | Path = None, buffer: bytes | str = None, on_bad_lines=\"skip\"\n",
    "):\n",
    "    \"\"\"\n",
    "    If you want to split CSVs in your notebook and so on when working\n",
    "    with Featrix, this function should be used to capture the extra work\n",
    "    around pandas' `pd.read_csv` that you'll want for best performance\n",
    "    with Featrix. We will add split and a way to get back the test df\n",
    "    to the client in a future release.\n",
    "\n",
    "    Any column with an 'int' type -- meaning there doesn't seem to be a\n",
    "    header line in the CSV -- will be renamed to `column_N`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the CSV on your local system.\n",
    "    buffer: str or bytes\n",
    "        The CSV already in buffer\n",
    "    on_bad_lines: str\n",
    "        What to do with bad lines. By default, we 'skip', but you may want to 'error'.\n",
    "        This is passed directly to `pd.read_csv`.\n",
    "\n",
    "    This can raise exceptions if the file is not found or seems to be empty.\n",
    "\n",
    "    \"\"\"\n",
    "    if not file_path and not buffer:\n",
    "        raise ValueError(\n",
    "            \"No data provided via buffer or path to featrix_wrap_pd_read_csv\"\n",
    "        )\n",
    "    if file_path:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"No such file {file_path}\")\n",
    "        # get the size of the file\n",
    "        sz = os.path.getsize(file_path)\n",
    "        if sz == 0:\n",
    "            raise Exception(f\"The file {file_path} appears to be 0 bytes long.\")\n",
    "    elif isinstance(buffer, bytes):\n",
    "        buffer = buffer.decode()\n",
    "    buffer_io = StringIO(buffer) if buffer else None\n",
    "    dialect = None\n",
    "    has_header = True\n",
    "\n",
    "    sniffer = csv.Sniffer()\n",
    "    if buffer:\n",
    "        dialect = sniffer.sniff(buffer)\n",
    "        has_header = sniffer.has_header(buffer)\n",
    "    else:\n",
    "        with open(file_path, newline=\"\", errors='ignore') as csvfile:\n",
    "            # For some very wide files, 2K isn't enough.\n",
    "            # It's possible 256K isn't either, but one has to draw the line!\n",
    "            try:\n",
    "                sample = csvfile.read(32 * 1024)\n",
    "            # except UnicodeDecodeError as err:\n",
    "                # print(\"bad unicode:\",dir(err))\n",
    "                # print(\"err.reason: \", err.reason)\n",
    "                # print(\"err.start: \", err.start)\n",
    "                # print(\"err.end: \", err.end)\n",
    "            except:\n",
    "                bad_line = _find_bad_line_number(file_path=file_path, buffer=buffer)\n",
    "                if bad_line > 0:\n",
    "                    print(\"first BAD LINE WAS ...\", bad_line)\n",
    "\n",
    "            dialect = sniffer.sniff(sample)\n",
    "            has_header = sniffer.has_header(sample)\n",
    "\n",
    "    csv_parameters = {\n",
    "        'delimiter': dialect.delimiter,\n",
    "        'quotechar': dialect.quotechar,\n",
    "        'escapechar': dialect.escapechar,\n",
    "        'doublequote': dialect.doublequote,\n",
    "        'skipinitialspace': dialect.skipinitialspace,\n",
    "        'quoting': dialect.quoting,\n",
    "        # Pandas does not support line terminators > 1 but Sniffer returns things like '\\r\\n'\n",
    "        # 'lineterminator': dialect.lineterminator\n",
    "    }\n",
    "\n",
    "    if has_header:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path or buffer_io,\n",
    "                # Pandas doesn't take the same dialect as csv.Sniffer produces so we create csv_parameters\n",
    "                # dialect=dialect,\n",
    "                on_bad_lines=on_bad_lines,\n",
    "                encoding_errors='ignore',\n",
    "                **csv_parameters\n",
    "            )\n",
    "        except csv.Error as err:\n",
    "            bad_line = _find_bad_line_number(file_path=file_path, buffer=buffer)\n",
    "            if bad_line > 0:\n",
    "                print(\"first BAD LINE WAS ...\", bad_line)\n",
    "            s_err = str(err)\n",
    "            print(s_err)\n",
    "            # FIXME: Not sure if there is something we can do if the buffer is hosed?\n",
    "            if (\n",
    "                s_err is not None\n",
    "                and s_err.find(\"malformed\") >= 0\n",
    "                and file_path is not None\n",
    "            ):\n",
    "                df = pd.read_csv(\n",
    "                    file_path,\n",
    "                    # Pandas doesn't take the same dialect as csv.Sniffer produces so we create csv_parameters\n",
    "                    # dialect=dialect,\n",
    "                    on_bad_lines=on_bad_lines,\n",
    "                    lineterminator=\"\\n\",\n",
    "                    **csv_parameters\n",
    "                )\n",
    "                print(\"recovered\")\n",
    "            else:\n",
    "                print(\"c'est la vie\")\n",
    "                raise err\n",
    "            # endif\n",
    "\n",
    "        # if any of the columns have an 'int' type, rename it.\n",
    "        if df is not None:\n",
    "            cols = list(df.columns)\n",
    "            renames = {}\n",
    "            for idx, c in enumerate(cols):\n",
    "                if not isinstance(c, str):\n",
    "                    renames[c] = \"column_\" + str(c)\n",
    "            if len(renames) > 0:\n",
    "                df.rename(columns=renames, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    if not has_header:\n",
    "        # Try again -- and see.\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path or buffer_io,  **csv_parameters)\n",
    "            cols = df.columns\n",
    "            if len(cols) >= 0:\n",
    "                if cols[0].startswith(\"Unnamed\"):\n",
    "                    # still no good.\n",
    "                    raise Exception(\n",
    "                        f\"CSV file {file_path} doesn't seem to have a header line, which means it does not \"\n",
    "                        \"have labels for the columns. This will make creating predictions on \"\n",
    "                        \"specific targets difficult!\"\n",
    "                    )\n",
    "            return df\n",
    "        except Exception as err:  # noqa - catch anything\n",
    "            traceback.print_exc()\n",
    "            raise Exception(\n",
    "                f\"CSV file {file_path} doesn't seem to have a header line, which means it does not \"\n",
    "                \"have labels for the columns. This will make creating predictions on specific targets difficult! [2]\"\n",
    "            )\n",
    "\n",
    "    return None\n"
   ],
   "id": "1b60dfd50377f6db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Code From UI Example ",
   "id": "625f3734ff724fc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "FEATRIX_CLIENT_ID     = os.environ.get('FEATRIX_CLIENT_ID')\n",
    "FEATRIX_CLIENT_SECRET = os.environ.get('FEATRIX_CLIENT_SECRET')\n",
    "\n",
    "def predict_Loan_Status(\n",
    "    input: Dict | List[Dict]\n",
    "):\n",
    "    \"\"\"\n",
    "    Call this function with a list of records or\n",
    "    a single record as a dictionary.\n",
    "\n",
    "    Returns an array of predictions.\n",
    "    \"\"\"\n",
    "    client = ft.networkclient.new_client(\n",
    "                'http://localhost:3001',  # DEBUG\n",
    "                client_id=FEATRIX_CLIENT_ID,\n",
    "                client_secret=FEATRIX_CLIENT_SECRET,\n",
    "                allow_unencrypted_http=True,  # DEBUG\n",
    "    )\n",
    "    nf_id = '66227cf75749129f0aa3ea20'\n",
    "    nf = client.get_neural_function(nf_id)\n",
    "    x = nf.predict(input)\n",
    "    return x\n",
    "\n",
    "\n",
    "print(predict_Loan_Status({'weight': 20}))\n"
   ],
   "id": "3e4fe08b71e0efbd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
